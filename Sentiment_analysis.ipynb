{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrutha2413/Sentiment-Analysis/blob/main/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BYA1iqMZHFCn",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df291da-5727-4263-b5d8-051a7fdba2f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "#Installs genism, a library used for topic modeling and word embeddings\n",
        "!pip install gensim pandas scikit-learn\n",
        "#imports the Word2Vec model, which learns vector representation of words\n",
        "from gensim.models import Word2Vec\n",
        "#import NumPy which is used for numerical operations and vectors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#imports regular expressions for text cleaning\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DYErNCAxHlB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "11c1fc3b-881f-451d-8873-a1b5adcf0745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['app_id', 'app_name', 'review_text', 'review_score', 'review_votes'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# ----------------------\n",
        "# Dataset --> the trainign dataset\n",
        "# Each item was a tuple so first value is the sentence and second value is the label (1 = positive sentiment, 0 = negative sentiment)\n",
        "# ----------------------\n",
        "df = pd.read_csv(\"/content/dataset.csv\")\n",
        "df.head()\n",
        "df.columns\n",
        "print(df.columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df[\"review_text\"]\n",
        "\n",
        "\n",
        "labels_raw = df[\"review_score\"]\n",
        "print(\"Unique values in df['review_score'] before mapping:\", labels_raw.unique())\n",
        "print(\"Value counts in df['review_score'] before mapping:\\n\", labels_raw.value_counts(dropna=False))\n",
        "\n",
        "# Map review_score to binary sentiment: 1 for positive (score 1), 0 for negative (score -1).\n",
        "# Also handles potential NaNs by mapping them to 0 (negative) if they somehow appear.\n",
        "labels = labels_raw.apply(lambda x: 1 if x == 1 else 0)\n",
        "print(\"\\nLabels after mapping:\\n\", labels.value_counts(dropna=False))\n",
        "\n",
        "labels = labels.astype(int)\n",
        "print(\"\\nLabels after astype(int):\\n\", labels.value_counts(dropna=False))\n",
        "\n",
        "sentences = list(zip(texts, labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgmSGcSH0lyr",
        "outputId": "27269e8c-cf49-4441-da2a-9d3e33c84775"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in df['review_score'] before mapping: [ 1 -1]\n",
            "Value counts in df['review_score'] before mapping:\n",
            " review_score\n",
            " 1    5260420\n",
            "-1    1156686\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Labels after mapping:\n",
            " review_score\n",
            "1    5260420\n",
            "0    1156686\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Labels after astype(int):\n",
            " review_score\n",
            "1    5260420\n",
            "0    1156686\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9nHouKmOHlUM"
      },
      "outputs": [],
      "source": [
        "#Defines a function to clean and split text into words\n",
        "def tokenize(text):\n",
        "    # Ensure text is a string before processing, converting non-strings (like floats) to an empty string\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    #coverts all the characters to lowercase\n",
        "    text = text.lower()\n",
        "    #removes anything that is not lowercase letter and any whitespaces\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    #splits the sentence into a list of words\n",
        "    return text.split()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train / test splitting\n",
        "X_texts = [text for text, _ in sentences]\n",
        "y = np.array([label for _, label in sentences])\n",
        "\n",
        "\n",
        "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
        "    X_texts,\n",
        "    y,\n",
        "    test_size = 0.2,\n",
        "    random_state = 42\n",
        ")\n"
      ],
      "metadata": {
        "id": "THNMHI1j0sRD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3Fuw18LHllV"
      },
      "outputs": [],
      "source": [
        "train_corpus = [tokenize('' if pd.isna(t) else t) for t in X_train_texts] # Tokenizes each text in 'X_train_texts', handling any NaN values by replacing them with an empty string before tokenizing. The result is a list of tokenized training reviews.\n",
        "test_corpus = [tokenize('' if pd.isna(t) else t) for t in X_test_texts] # Tokenizes each text in 'X_test_texts', handling any NaN values by replacing them with an empty string before tokenizing. The result is a list of tokenized testing reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDGwPZ53HpdS"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Train Word2Vec\n",
        "# ----------------------\n",
        "\n",
        "model = Word2Vec(train_corpus, vector_size=50, window=4, min_count=2, sg=1)\n",
        "# train_corpus: The list of tokenized sentences to learn from.\n",
        "# vector_size=50: Each word will be represented by a 50-dimensional vector.\n",
        "# window=4: Considers 4 words before and 4 words after the current word for context.\n",
        "# min_count=2: Ignores all words with a total frequency lower than 2.\n",
        "# sg=1: Uses the Skip-gram model (sg=1) over CBOW (sg=0), which is generally better for capturing semantic relationships."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Sentence vector\n",
        "# ----------------------\n",
        "def sentence_vector(tokens): # Defines a function that takes a list of 'tokens' (words) and converts them into a single numerical vector representing the sentence's meaning.\n",
        "\n",
        "    vecs = [model.wv[w] for w in tokens if w in model.wv] # Iterates through each token in the input list. For each token, it checks if the word exists in the Word2Vec model's vocabulary (model.wv).\n",
        "    # If the word exists, it retrieves its corresponding vector from the Word2Vec model. These vectors are collected into a list called 'vecs'.\n",
        "\n",
        "    if not vecs: # Checks if the 'vecs' list is empty. This happens if none of the words in the sentence were found in the Word2Vec model's vocabulary.\n",
        "        return np.zeros(model.vector_size) # If 'vecs' is empty, it returns a NumPy array of zeros with the same dimension as the Word2Vec model's vectors. This acts as a placeholder for sentences with no recognized words.\n",
        "\n",
        "    return np.mean(vecs, axis=0) # If 'vecs' is not empty, it calculates the mean (average) of all the word vectors in the 'vecs' list along the 0-axis (column-wise). This effectively combines all individual word vectors into a single, aggregated sentence vector."
      ],
      "metadata": {
        "id": "8JGOvVUEXKfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([sentence_vector(t) for t in train_corpus]) # Converts each tokenized training review in 'train_corpus' into a numerical sentence vector using the 'sentence_vector' function and stores them as a NumPy array 'X_train'.\n",
        "X_test = np.array([sentence_vector(t) for t in test_corpus]) # Converts each tokenized testing review in 'test_corpus' into a numerical sentence vector using the 'sentence_vector' function and stores them as a NumPy array 'X_test'."
      ],
      "metadata": {
        "id": "cX200jloXMt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Simple sentiment prototypes\n",
        "# ----------------------\n",
        "pos_vec = np.mean(X_train[y_train == 1], axis=0) # Calculates the mean vector for all training reviews labeled as positive (where y_train is 1), creating a 'positive sentiment prototype'.\n",
        "neg_vec = np.mean(X_train[y_train == 0], axis=0) # Calculates the mean vector for all training reviews labeled as negative (where y_train is 0), creating a 'negative sentiment prototype'."
      ],
      "metadata": {
        "id": "CdeE4bOWXO15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine(a, b): # Defines a function to calculate the cosine similarity between two vectors 'a' and 'b'.\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)) # Computes the dot product of 'a' and 'b' and divides by the product of their L2 norms, effectively measuring the cosine of the angle between them.\n",
        "\n",
        "def predict_vector(v): # Defines a function that predicts the sentiment label (0 or 1) for a given sentence vector 'v'.\n",
        "    return 1 if cosine(v, pos_vec) > cosine(v, neg_vec) else 0 # Returns 1 (positive) if 'v' is more similar to the positive prototype ('pos_vec') than to the negative prototype ('neg_vec'), otherwise returns 0 (negative).\n",
        "\n",
        "\n",
        "def predict(sentence): # Defines a function that takes a raw 'sentence' string and predicts its sentiment as \"positive\" or \"negative\".\n",
        "    v = sentence_vector(tokenize(sentence)) # Converts the input 'sentence' into a numerical vector by first tokenizing it and then applying the 'sentence_vector' function.\n",
        "    return \"positive\" if predict_vector(v) == 1 else \"negative\" # Returns \"positive\" if the sentence's vector is classified as 1 (positive) by 'predict_vector', otherwise returns \"negative\"."
      ],
      "metadata": {
        "id": "xFUcS9FlXUdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy Evaluation\n",
        "y_pred = np.array([predict_vector(v) for v in X_test]) # Generates predictions for each sentence vector in the test set (X_test) using the 'predict_vector' function.\n",
        "accuracy = accuracy_score(y_test, y_pred) # Calculates the accuracy of the model by comparing the predicted labels (y_pred) against the true labels (y_test).\n",
        "print(\"Model accuracy:\", accuracy) # Prints the calculated accuracy score of the model to the console.\n"
      ],
      "metadata": {
        "id": "w64WYuZEp4WR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeDDwYL42h5Dh3K9aHQnmD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}