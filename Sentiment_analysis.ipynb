{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrutha2413/Sentiment-Analysis/blob/main/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BYA1iqMZHFCn",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0198bc6c-ef89-4d13-a471-33c3391c0341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "#Installs genism, a library used for topic modeling and word embeddings\n",
        "!pip install gensim pandas scikit-learn\n",
        "#imports the Word2Vec model, which learns vector representation of words\n",
        "from gensim.models import Word2Vec\n",
        "#import NumPy which is used for numerical operations and vectors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#imports regular expressions for text cleaning\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DYErNCAxHlB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b084cf36-79e6-42de-8c9d-ea5e8cb0c866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'recommendationid', 'language', 'review',\n",
            "       'timestamp_created', 'timestamp_updated', 'voted_up', 'votes_up',\n",
            "       'votes_funny', 'weighted_vote_score', 'comment_count', 'steam_purchase',\n",
            "       'received_for_free', 'written_during_early_access', 'author.steamid',\n",
            "       'author.num_games_owned', 'author.num_reviews',\n",
            "       'author.playtime_forever', 'author.playtime_last_two_weeks',\n",
            "       'author.playtime_at_review', 'author.last_played'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# ----------------------\n",
        "# Dataset --> the trainign dataset\n",
        "# Each item was a tuple so first value is the sentence and second value is the label (1 = positive sentiment, 0 = negative sentiment)\n",
        "# ----------------------\n",
        "df = pd.read_csv(\"/content/dataset.csv\")\n",
        "df.head()\n",
        "df.columns\n",
        "print(df.columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df[\"review\"]\n",
        "\n",
        "\n",
        "labels_raw = df[\"voted_up\"]\n",
        "print(\"Unique values in df['voted_up'] before mapping:\", labels_raw.unique())\n",
        "print(\"Value counts in df['voted_up'] before mapping:\\n\", labels_raw.value_counts(dropna=False))\n",
        "\n",
        "# Map voted_up to binary sentiment: 1 for positive (True), 0 for negative (False).\n",
        "# The .astype(int) method will convert True to 1 and False to 0.\n",
        "labels = labels_raw.astype(int)\n",
        "print(\"\\nLabels after mapping (True=1, False=0):\\n\", labels.value_counts(dropna=False))\n",
        "\n",
        "sentences = list(zip(texts, labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgmSGcSH0lyr",
        "outputId": "95e3b914-2924-48e3-d8e6-b1470630e10c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in df['voted_up'] before mapping: [ True False]\n",
            "Value counts in df['voted_up'] before mapping:\n",
            " voted_up\n",
            "True     3230\n",
            "False     467\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Labels after mapping (True=1, False=0):\n",
            " voted_up\n",
            "1    3230\n",
            "0     467\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9nHouKmOHlUM"
      },
      "outputs": [],
      "source": [
        "#Defines a function to clean and split text into words\n",
        "def tokenize(text):\n",
        "    # Ensure text is a string before processing, converting non-strings (like floats) to an empty string\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    #coverts all the characters to lowercase\n",
        "    text = text.lower()\n",
        "    #removes anything that is not lowercase letter and any whitespaces\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    #splits the sentence into a list of words\n",
        "    return text.split()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train / test splitting\n",
        "X_texts = [text for text, _ in sentences]\n",
        "y = np.array([label for _, label in sentences])\n",
        "\n",
        "\n",
        "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
        "    X_texts,\n",
        "    y,\n",
        "    test_size = 0.2,\n",
        "    random_state = 42\n",
        ")\n"
      ],
      "metadata": {
        "id": "THNMHI1j0sRD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h3Fuw18LHllV"
      },
      "outputs": [],
      "source": [
        "train_corpus = [tokenize('' if pd.isna(t) else t) for t in X_train_texts] # Tokenizes each text in 'X_train_texts', handling any NaN values by replacing them with an empty string before tokenizing. The result is a list of tokenized training reviews.\n",
        "test_corpus = [tokenize('' if pd.isna(t) else t) for t in X_test_texts] # Tokenizes each text in 'X_test_texts', handling any NaN values by replacing them with an empty string before tokenizing. The result is a list of tokenized testing reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SDGwPZ53HpdS"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Train Word2Vec\n",
        "# ----------------------\n",
        "\n",
        "model = Word2Vec(train_corpus, vector_size=50, window=4, min_count=2, sg=1)\n",
        "# train_corpus: The list of tokenized sentences to learn from.\n",
        "# vector_size=50: Each word will be represented by a 50-dimensional vector.\n",
        "# window=4: Considers 4 words before and 4 words after the current word for context.\n",
        "# min_count=2: Ignores all words with a total frequency lower than 2.\n",
        "# sg=1: Uses the Skip-gram model (sg=1) over CBOW (sg=0), which is generally better for capturing semantic relationships."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Sentence vector\n",
        "# ----------------------\n",
        "def sentence_vector(tokens): # Defines a function that takes a list of 'tokens' (words) and converts them into a single numerical vector representing the sentence's meaning.\n",
        "\n",
        "    vecs = [model.wv[w] for w in tokens if w in model.wv] # Iterates through each token in the input list. For each token, it checks if the word exists in the Word2Vec model's vocabulary (model.wv).\n",
        "    # If the word exists, it retrieves its corresponding vector from the Word2Vec model. These vectors are collected into a list called 'vecs'.\n",
        "\n",
        "    if not vecs: # Checks if the 'vecs' list is empty. This happens if none of the words in the sentence were found in the Word2Vec model's vocabulary.\n",
        "        return np.zeros(model.vector_size) # If 'vecs' is empty, it returns a NumPy array of zeros with the same dimension as the Word2Vec model's vectors. This acts as a placeholder for sentences with no recognized words.\n",
        "\n",
        "    return np.mean(vecs, axis=0) # If 'vecs' is not empty, it calculates the mean (average) of all the word vectors in the 'vecs' list along the 0-axis (column-wise). This effectively combines all individual word vectors into a single, aggregated sentence vector."
      ],
      "metadata": {
        "id": "8JGOvVUEXKfI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([sentence_vector(t) for t in train_corpus]) # Converts each tokenized training review in 'train_corpus' into a numerical sentence vector using the 'sentence_vector' function and stores them as a NumPy array 'X_train'.\n",
        "X_test = np.array([sentence_vector(t) for t in test_corpus]) # Converts each tokenized testing review in 'test_corpus' into a numerical sentence vector using the 'sentence_vector' function and stores them as a NumPy array 'X_test'."
      ],
      "metadata": {
        "id": "cX200jloXMt4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Simple sentiment prototypes\n",
        "# ----------------------\n",
        "pos_vec = np.mean(X_train[y_train == 1], axis=0) # Calculates the mean vector for all training reviews labeled as positive (where y_train is 1), creating a 'positive sentiment prototype'.\n",
        "neg_vec = np.mean(X_train[y_train == 0], axis=0) # Calculates the mean vector for all training reviews labeled as negative (where y_train is 0), creating a 'negative sentiment prototype'."
      ],
      "metadata": {
        "id": "CdeE4bOWXO15"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine(a, b): # Defines a function to calculate the cosine similarity between two vectors 'a' and 'b'.\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)) # Computes the dot product of 'a' and 'b' and divides by the product of their L2 norms, effectively measuring the cosine of the angle between them.\n",
        "\n",
        "def predict_vector(v): # Defines a function that predicts the sentiment label (0 or 1) for a given sentence vector 'v'.\n",
        "    return 1 if cosine(v, pos_vec) > cosine(v, neg_vec) else 0 # Returns 1 (positive) if 'v' is more similar to the positive prototype ('pos_vec') than to the negative prototype ('neg_vec'), otherwise returns 0 (negative).\n",
        "\n",
        "\n",
        "def predict(sentence): # Defines a function that takes a raw 'sentence' string and predicts its sentiment as \"positive\" or \"negative\".\n",
        "    v = sentence_vector(tokenize(sentence)) # Converts the input 'sentence' into a numerical vector by first tokenizing it and then applying the 'sentence_vector' function.\n",
        "    return \"positive\" if predict_vector(v) == 1 else \"negative\" # Returns \"positive\" if the sentence's vector is classified as 1 (positive) by 'predict_vector', otherwise returns \"negative\"."
      ],
      "metadata": {
        "id": "xFUcS9FlXUdb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy Evaluation\n",
        "y_pred = np.array([predict_vector(v) for v in X_test]) # Generates predictions for each sentence vector in the test set (X_test) using the 'predict_vector' function.\n",
        "accuracy = accuracy_score(y_test, y_pred) # Calculates the accuracy of the model by comparing the predicted labels (y_pred) against the true labels (y_test).\n",
        "print(\"Model accuracy:\", accuracy) # Prints the calculated accuracy score of the model to the console.\n"
      ],
      "metadata": {
        "id": "w64WYuZEp4WR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b589405-60b5-4952-ad4f-95a3eb59423b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 0.5783783783783784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-823121179.py:2: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)) # Computes the dot product of 'a' and 'b' and divides by the product of their L2 norms, effectively measuring the cosine of the angle between them.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM2rzx6/6PhVjc+Xs5auFi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}